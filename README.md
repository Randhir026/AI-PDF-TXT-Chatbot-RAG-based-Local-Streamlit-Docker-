# AI-PDF-TXT-Chatbot-RAG-based-Local-Streamlit-Docker-
This project is a Retrieval-Augmented Generation (RAG) based chatbot that lets you upload PDF or TXT documents and ask questions about their content â€” all running locally (âš¡ no API keys or internet dependency required after setup).

---

## ğŸš€ Features

- ğŸ“‚ Upload **PDF or TXT** documents
- ğŸ§  Automatically chunk and embed text
- ğŸ” Retrieve most relevant content using **FAISS**
- ğŸ’¬ Generate answers locally using **open-source models**
- âš™ï¸ Fully **offline** â€” no OpenAI or external API required
- ğŸ–¥ï¸ Clean and responsive **Streamlit UI**

---

## ğŸ“ Folder Structure
pdf-chatbot/

â”‚

â”œâ”€â”€ app/

â”‚   â”œâ”€â”€ main.py                     â† Streamlit app (UI)

â”‚   â””â”€â”€ src/

â”‚       â”œâ”€â”€ embedding_utils.py      â† Embeddings + FAISS

â”‚       â”œâ”€â”€ generator.py            â† Local generator (DistilGPT2)

â”‚       â””â”€â”€ pdf_utils.py            â† PDF text chunking

â”‚

â”œâ”€â”€ chunks/                         â† Stores document chunks

â”œâ”€â”€ vectordb/                       â† Stores FAISS index

â”‚

â”œâ”€â”€ requirements.txt                â† Python dependencies

â”œâ”€â”€ Dockerfile                      â† Build environment

â”œâ”€â”€ docker-compose.yml              â† Run services

â”œâ”€â”€ .env                            â† Environment config (port)

â””â”€â”€ README.md


## ğŸš€ How to Run (Step-by-Step)

 - 1ï¸âƒ£ Clone the repository
    git clone https://github.com/your-username/pdf-chatbot.git
    cd pdf-chatbot
- 2ï¸âƒ£ Build the Docker image
    docker compose build
- 3ï¸âƒ£ Run the chatbot
    docker compose up
  
## âš™ï¸ Configuration
 - You can adjust default settings in the .env file:
   
               STREAMLIT_SERVER_PORT=8501
   
## ğŸ§© How It Works

- Upload a PDF or TXT file
   - The system extracts and splits text into manageable chunks.
   - Each chunk is embedded using a local transformer model.
   - A FAISS index stores embeddings for efficient search.
   - When you ask a question:
        - Relevant chunks are retrieved.
        - The local LLM model generates an answer from context.

## ğŸ“œ License
   This project is released under the MIT License.
   
   You are free to use, modify, and distribute it with attribution.
 
## ğŸ‘¨â€ğŸ’» Author
Randhir Kumar

ğŸ“§randhirkumar015@gmail.com

ğŸ’¼ AI/ML & Data Science Enthusiast
